{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "DcJb_l8fjEC4",
        "oqBmNVzclmpa",
        "Wol2mq6nMxo1",
        "ran8ZjTcNz7E",
        "aH_N9h87N2i-",
        "V-DpLvxywaEQ",
        "0ln2xw_fhxOS",
        "6rvYJdtPkI-i",
        "oExGPlYOlwcD",
        "hbCHy23Jnv6c",
        "S1mYFX_coTQ3",
        "v2qlW7VCoXjk",
        "4wHCG18Bo0ZO",
        "xVyTOkm5pysO",
        "E_AiAowYrngg",
        "KLPVrsNytKYm",
        "etLH5PmLuxoK",
        "GxuEP-7SxCdb",
        "CVa86Ory85Dq",
        "BFQxTfcQAuVM",
        "z1QZfO0gCb_g",
        "8RwWtasIGBFG",
        "5uVLZd0cC66o",
        "PHhmYdK1InVO",
        "H3QTvvUdDjSA",
        "2h9CAv9qtgDC",
        "0pLBZNnHJzqS",
        "SjgxyUt4H5DQ",
        "U1lYBXnoIC-H"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%cd -"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3RVpjv3rekB",
        "outputId": "477bdac7-f2d1-431a-efb1-b663b7af1e59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2fBHdThrsSa",
        "outputId": "661699a1-fd53-48f3-c779-080efd19b715"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/ams_595_python_teaching"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGKmId6xWUrX",
        "outputId": "a9f3f3f6-245b-421d-ad55-2c3a42a7da71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ams_595_python_teaching\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lecture 8&9 Introduction to Scientific Computing Part 2: Numerical Algorithms**#"
      ],
      "metadata": {
        "id": "BFQxTfcQAuVM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Commonly Seen Numerical Methods"
      ],
      "metadata": {
        "id": "z1QZfO0gCb_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a list (far away from complete) of common numerical methods. We will briefly talk about some of these in this lecture. Your will be asked to implement some of these algorithms in your assginment."
      ],
      "metadata": {
        "id": "PMnPyxspCk0I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Root-Finding Methods**:\n",
        "   - *Bisection Method*: Iteratively narrow down the interval containing a root by bisecting it.\n",
        "   - *Newton-Raphson Method*: Uses derivatives to iteratively refine an approximation to a root.\n",
        "   - *Secant Method*: An iterative method for approximating roots without requiring derivative information.\n",
        "\n",
        "2. **Linear Algebra Methods**:\n",
        "   - *Gaussian Elimination*: Solves systems of linear equations through row operations.\n",
        "   - *LU Decomposition*: Decomposes a matrix into lower and upper triangular matrices to solve linear systems.\n",
        "   - *Iterative Methods (e.g., Jacobi, Gauss-Seidel)*: Solves linear systems iteratively by updating approximations.\n",
        "\n",
        "3. **Interpolation and Approximation**:\n",
        "   - *Lagrange Interpolation*: Constructs a polynomial that passes through given data points.\n",
        "   - *Newton's Divided Difference Interpolation*: A method for polynomial interpolation.\n",
        "   - *Least Squares Approximation*: Minimizes the sum of the squares of the residuals to approximate data.\n",
        "\n",
        "4. **Numerical Differentiation and Integration**:\n",
        "   - *Numerical Integration (e.g., Trapezoidal Rule, Simpson's Rule)*: Approximates definite integrals.\n",
        "   - *Differentiation (e.g., Finite Difference Methods)*: Estimates derivatives from discrete data.\n",
        "\n",
        "5. **Ordinary Differential Equations (ODEs)**:\n",
        "   - *Euler's Method*: Approximates solutions to first-order ODEs.\n",
        "   - *Runge-Kutta Methods*: Higher-order ODE solvers, e.g., RK4.\n",
        "   - *Boundary Value Problems (e.g., Shooting Method)*: Solves ODEs with specified boundary conditions.\n",
        "\n",
        "6. **Partial Differential Equations (PDEs)**:\n",
        "   - *Finite Difference Method*: Discretizes PDEs in space and time.\n",
        "   - *Finite Element Method (FEM)*: Decomposes the domain into elements for PDE solution.\n",
        "   - *Finite Volume Method (FVM)*: Focuses on conservations laws for PDEs.\n",
        "\n",
        "7. **Optimization Methods**:\n",
        "   - *Gradient Descent*: An iterative method for optimizing functions, often used in machine learning.\n",
        "   - *Newton's Method for Optimization*: An iterative method for minimizing or maximizing functions.\n",
        "   - *Genetic Algorithms*: Optimization inspired by natural selection and genetics.\n",
        "\n",
        "8. **Monte Carlo Methods**:\n",
        "   - *Monte Carlo Integration*: Approximates integrals by random sampling.\n",
        "   - *Markov Chain Monte Carlo (MCMC)*: Generates samples from complex distributions.\n",
        "   - *Simulated Annealing*: An optimization method inspired by annealing processes in metallurgy.\n",
        "\n",
        "9. **Numerical Linear Algebra**:\n",
        "   - *Singular Value Decomposition (SVD)*: Decomposes a matrix into three other matrices and is used in data compression and dimensionality reduction.\n",
        "   - *Principal Component Analysis (PCA)*: A dimensionality reduction technique based on SVD.\n",
        "   - *Eigenvalue and Eigenvector Computation*: Used in various applications, including quantum mechanics and vibration analysis."
      ],
      "metadata": {
        "id": "lq9v1IATCS_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Root Finding"
      ],
      "metadata": {
        "id": "8RwWtasIGBFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bisection Method:\n",
        "\n",
        "![link text](https://www.researchgate.net/publication/336638575/figure/fig2/AS:815189725286401@1571367784538/Bisection-method-This-Bisection-method-states-that-if-fx-is-continuous-which-is-defined.ppm)\n",
        "\n",
        "Explore Newton's method and the Secant method on your own!"
      ],
      "metadata": {
        "id": "NRx27BU4GD4t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Strassen for Matrix Multiplication"
      ],
      "metadata": {
        "id": "5uVLZd0cC66o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Strassen algorithm is a divide-and-conquer technique used for efficient matrix multiplication. It reduces the number of basic multiplications required for multiplying two matrices, resulting in faster matrix multiplication for large matrices. The algorithm was developed by Volker Strassen in 1969."
      ],
      "metadata": {
        "id": "Ce8ER0yVC8sx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Divide\n",
        "\n",
        "1. Divide the two input matrices, A and B, into four equally sized submatrices. Let's call these submatrices A11, A12, A21, A22, B11, B12, B21, and B22.\n",
        "\n",
        "Step 2: Recurse\n",
        "\n",
        "2. Recursively compute seven products of submatrices (P1 to P7) using the following formulas:\n",
        "   - P1 = A11 * (B12 - B22)\n",
        "   - P2 = (A11 + A12) * B22\n",
        "   - P3 = (A21 + A22) * B11\n",
        "   - P4 = A22 * (B21 - B11)\n",
        "   - P5 = (A11 + A22) * (B11 + B22)\n",
        "   - P6 = (A12 - A22) * (B21 + B22)\n",
        "   - P7 = (A11 - A21) * (B11 + B12)\n",
        "\n",
        "Step 3: Combine\n",
        "\n",
        "3. Calculate the four submatrices of the result matrix, C, using the products computed in the previous step:\n",
        "   - C11 = P5 + P4 - P2 + P6\n",
        "   - C12 = P1 + P2\n",
        "   - C21 = P3 + P4\n",
        "   - C22 = P5 + P1 - P3 - P7\n",
        "\n",
        "Step 4: Recursion Termination\n",
        "\n",
        "4. If the submatrices become small enough (below a certain threshold), perform standard matrix multiplication to calculate C11, C12, C21, and C22."
      ],
      "metadata": {
        "id": "9fIxBawQDJv5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Jacobi Method for $Ax=b$"
      ],
      "metadata": {
        "id": "PHhmYdK1InVO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Jacobi method is an iterative algorithm used to solve a system of linear equations, particularly when the system is diagonally dominant or when it can be transformed into a diagonally dominant system. It is one of several iterative methods used for solving such systems, along with methods like Gauss-Seidel and successive over-relaxation (SOR)."
      ],
      "metadata": {
        "id": "NgLqaKwqIuKV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Matrix-based formula**\n",
        "\n",
        "Then $A$ can be decomposed into a diagonal component $D$, a lower triangular part $L$ and an upper triangular part $U$ :\n",
        "$$\n",
        "A=D+L+U \\quad \\text { where } \\quad D=\\left[\\begin{array}{cccc}\n",
        "a_{11} & 0 & \\cdots & 0 \\\\\n",
        "0 & a_{22} & \\cdots & 0 \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "0 & 0 & \\cdots & a_{n n}\n",
        "\\end{array}\\right] \\text { and } L+U=\\left[\\begin{array}{cccc}\n",
        "0 & a_{12} & \\cdots & a_{1 n} \\\\\n",
        "a_{21} & 0 & \\cdots & a_{2 n} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "a_{n 1} & a_{n 2} & \\cdots & 0\n",
        "\\end{array}\\right] \\text {. }\n",
        "$$\n",
        "The solution is then obtained iteratively via\n",
        "$$\n",
        "\\mathbf{x}^{(k+1)}=D^{-1}\\left(\\mathbf{b}-(L+U) \\mathbf{x}^{(k)}\\right) .\n",
        "$$\n",
        "\n",
        "**Element-based formula**\n",
        "\n",
        "The element-based formula for each row $i$ is thus:\n",
        "$$\n",
        "x_i^{(k+1)}=\\frac{1}{a_{i i}}\\left(b_i-\\sum_{j \\neq i} a_{i j} x_j^{(k)}\\right), \\quad i=1,2, \\ldots, n .\n",
        "$$\n",
        "The computation of $x_i^{(k+1)}$ requires each element in $\\mathbf{x}^{(k)}$ except itself. Unlike the Gauss-Seidel method, we can't overwrite $x_i^{(k)}$ with $x_i^{(k+1)}$, as that value will be needed by the rest of the computation.\n",
        "\n",
        "Explore Gauss-Seidel on your own!"
      ],
      "metadata": {
        "id": "ffe5NzYiI0QQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Numerical Integration"
      ],
      "metadata": {
        "id": "H3QTvvUdDjSA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Numerical integration, also known as numerical quadrature, is a technique used to approximate the definite integral of a function when an analytical solution is either challenging or impossible. This approach is essential in various fields such as mathematics, physics, engineering, and computer science."
      ],
      "metadata": {
        "id": "wVysly7WDt0P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Riemman Sum:\n",
        "\n",
        "![link text](https://patrickwalls.github.io/mathematicalpython/integration/img/riemann-sums_3_0.png)\n",
        "\n",
        "![link text](https://patrickwalls.github.io/mathematicalpython/integration/img/trapezoid-rule_21_0.png)\n",
        "\n",
        "\n",
        "\n",
        "Image Source: Mathematical Python\n",
        "\n",
        "There are also other commonly used schemes for numerical integration. We will not discuss in this class.\n",
        "\n",
        "1. Simpson's rule is a higher-precision numerical integration method that uses quadratic approximations to the function in each subinterval. It provides even more accurate results compared to the Trapezoidal rule, especially for smoother functions.\n",
        "\n",
        "2. Gaussian quadrature is a numerical integration technique that uses weighted sum of function values at specific points within the interval. It provides highly precise estimates for a wide range of functions.\n",
        "\n",
        "Explore Monte Carlo Integration on your own!"
      ],
      "metadata": {
        "id": "kPIWspFOro3X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Numerical Differentiation and Finite Difference Method"
      ],
      "metadata": {
        "id": "2h9CAv9qtgDC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition of derivatives:\n",
        "$$f^{\\prime}(x) \\equiv \\lim _{h \\rightarrow 0} \\frac{f(x+h)-f(x)}{h}$$\n"
      ],
      "metadata": {
        "id": "NNomx8K9uWaj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward difference:\n",
        "$$f^{\\prime}(x) \\approx \\frac{f(x+h)-f(x)}{h}, \\text{ for some small $h$}$$\n",
        "\n",
        "Backward difference:\n",
        "$$f^{\\prime}(x) \\approx \\frac{f(x)-f(x-h)}{h}, \\text{ for some small $h$}$$\n",
        "\n",
        "Centeral difference:\n",
        "$$f^{\\prime}(x) \\approx \\frac{f(x+h)-f(x-h)}{2 h}$$\n",
        "\n",
        "Finite difference:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "f_x(x, y) & \\approx \\frac{f(x+h, y)-f(x-h, y)}{2 h} \\\\\n",
        "f_y(x, y) & \\approx \\frac{f(x, y+k)-f(x, y-k)}{2 k} \\\\\n",
        "f_{x x}(x, y) & \\approx \\frac{f(x+h, y)-2 f(x, y)+f(x-h, y)}{h^2} \\\\\n",
        "f_{y y}(x, y) & \\approx \\frac{f(x, y+k)-2 f(x, y)+f(x, y-k)}{k^2} \\\\\n",
        "f_{x y}(x, y) & \\approx \\frac{f(x+h, y+k)-f(x+h, y-k)-f(x-h, y+k)+f(x-h, y-k)}{4 h k}\n",
        "\\end{aligned}\n",
        "$$"
      ],
      "metadata": {
        "id": "ll_Pq6aeuk7p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Euler's Method for ODEs"
      ],
      "metadata": {
        "id": "0pLBZNnHJzqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider a general first order IVP:\n",
        "$$\\frac{{dy}}{{dt}} = f\\left( {t,y} \\right)\\hspace{0.25in}y\\left( {{t_0}} \\right) = {y_0}$$\n",
        "write down the equation of the tangent line to the solution at $t = t_0$. The tangent line is\n",
        "$$y = {y_0} + f\\left( {{t_0},{y_0}} \\right)\\left( {t - {t_0}} \\right)$$\n",
        "![link text](https://tutorial.math.lamar.edu/classes/de/EulersMethod_Files/image001.png)\n",
        "\n",
        "For $y_1 = y(t_1), t_1 = t_0 + h,$ for some $h>0$:\n",
        "$${y_1} = {y_0} + f\\left( {{t_0},{y_0}} \\right)\\left( {{t_1} - {t_0}} \\right)$$\n",
        "\n",
        "To accurately approximate $y_1$, we take a small $h$.\n",
        "\n",
        "We can continue in this fashion. Use the previously computed approximation to get the next approximation."
      ],
      "metadata": {
        "id": "AhavG1X97VfG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finite Difference for PDEs"
      ],
      "metadata": {
        "id": "kMKKt0129NPu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not so rigorous definition of PDEs: multivariate equations with derivatives of >1 dependent variables (ie, derivatives in equation are partial derivatives).\n",
        "\n",
        "Two common techniques:\n",
        "1. Finite Difference Methods: approximate derivatives by finite difference.\n",
        "2. Finite Element Methods: write the solution as a linear combination of simple basis functions.\n",
        "\n",
        "Main Idea: Estimate the derivatives using their finite-difference formulas within a discretized area. FDM can offer reliable estimations with theorectical guranttees and error bounds.\n",
        "\n",
        "By substituting ODE/PDE derivatives with FD formulas, the equations are transformed into algebraic forms. If the initial ODE/PDE is linear, the resulting algebra is also linear.\n",
        "\n",
        "Steps of the Finite Difference Method:\n",
        "1. Discretization of the Domain\n",
        "\n",
        "\n",
        "2. Replace the derivatives in the PDE with finite difference approximations.\n",
        "\n",
        "\n",
        "3. Formulate the discrete equations using the finite difference approximations.\n",
        "This step results in a system of algebraic equations.\n",
        "\n",
        "4. Solution of the algebraic system using numerical techniques like iterative solvers, direct methods, or matrix decompositions.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6Xwb8L2tAjnZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Concrete Example with 2D Poisson Equation"
      ],
      "metadata": {
        "id": "OrHabIzYCYPL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider a 2D Poisson's equation with Dirichlet boundary conditions:\n",
        "\\begin{eqnarray*}\n",
        "    \\Delta u(x, y) = f(x, y),& \\quad& \\text { in } \\Omega:=(0, 1) \\times (0, 1), \\\\\n",
        "    u(x, y) = g(x, y),& \\quad&  \\text { on } \\partial \\Omega,\n",
        "\\end{eqnarray*}\n",
        "where $\\Delta$ is the Laplacian operator, $f(x) = \\sin(x-y)$, and $g(x, y) = 1$."
      ],
      "metadata": {
        "id": "_hTrqAlQCdFL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The unit square domain can be discretized as an $N \\times N$ grid:\n",
        "$$x_i = i\\cdot h,~ y_j = j\\cdot h,~i, j \\in[0, N],$$\n",
        "where the step size $h = dx = dy = \\frac{1}{N}$, and\n",
        "$$u_{i, j}=u\\left(x_i, y_j\\right), ~f_{i, j}=f\\left(x_i, y_j\\right) = sin(x_i - y_j).$$\n",
        "2. By central difference approximation:\n",
        "\n",
        "![link text](https://raw.githubusercontent.com/wenhangao21/AMS595-Teaching/main/stencil.png)\n",
        "\n",
        "$$\\frac{u_{i+1, j}+u_{i-1, j}-2 u_{i, j}}{h^2}+\\frac{u_{i, j+1}+u_{i, j-1}-2 u_{i, j}}{h^2}=\\sin \\left(x_i-y_j\\right).$$\n",
        "3. For all unknown $u_{i,j}, i, j \\in[1, N-1]$, we have a system of $(N-2)^2$ equations with $(N-2)^2$ unknown variables, which can be written in the form of $Au = b$.\n",
        "4. Solve the algebraic system by numerical methods."
      ],
      "metadata": {
        "id": "SMPEkqACCqaz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution by Walframe:\n",
        "![link text](https://raw.githubusercontent.com/wenhangao21/AMS595-Teaching/main/wolframe_out.png)"
      ],
      "metadata": {
        "id": "EeuP1ZVGFAZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convergence by the Jacobi Method\n",
        "![link text](https://raw.githubusercontent.com/wenhangao21/AMS595-Teaching/main/conv_zero.png)"
      ],
      "metadata": {
        "id": "Mj6p66uREh1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "assignment compare the speed up between lambdify and direct numerical evaluation\n",
        "\n",
        "\n",
        "\n",
        "The speedup when using \"lambdified\" functions instead of direct numerical evaluation can be significant, often several orders of magnitude. Even in this simple example we get a significant speed up:"
      ],
      "metadata": {
        "id": "sq_7EY8QnZRD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lecture 9 Introduction to Scientific Machine Learning**#"
      ],
      "metadata": {
        "id": "sRUKHtFgHdVn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scientific Machine Learning (SciML) is an emerging interdisciplinary field that integrates scientific computing, numerical analysis, and machine learning techniques to solve complex scientific problems. By leveraging the power of machine learning in conjunction with the rigor of scientific methodologies, SciML aims to enhance our understanding of physical, biological, and chemical systems, and to facilitate the discovery of new scientific insights."
      ],
      "metadata": {
        "id": "CcKzL9BeHhSf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fundamentals of SciML"
      ],
      "metadata": {
        "id": "SjgxyUt4H5DQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Integration of Domain Knowledge: The fusion of domain-specific knowledge and machine learning techniques is crucial in SciML. Understanding the underlying scientific principles of the problem at hand is essential to build interpretable and accurate models.\n",
        "\n",
        "2. Data-driven Scientific Inquiry: SciML emphasizes the use of data-driven approaches to tackle scientific challenges. By employing techniques such as data assimilation and model calibration, scientists can leverage available data to refine and validate computational models.\n",
        "\n",
        "3. Hybrid Models: Hybrid models that combine mechanistic, physics-based models with data-driven machine learning models are common in SciML. This integration allows for the incorporation of domain knowledge while taking advantage of the capacity of machine learning to handle complex patterns in data.\n",
        "\n",
        "4. Uncertainty Quantification: Given the inherent uncertainties in scientific data and models, SciML places significant emphasis on uncertainty quantification. Techniques such as probabilistic modeling, Bayesian inference, and uncertainty propagation are employed to assess and manage uncertainties in predictions."
      ],
      "metadata": {
        "id": "5WURDjKqH8w3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Residual Model for PDEs"
      ],
      "metadata": {
        "id": "U1lYBXnoIC-H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the following boundary value problem for simplicity to introduce the deep-learning-based least squares method:\n",
        "\n",
        "Find the unknown solution $u(\\boldsymbol{\\boldsymbol{x}})$ such that\n",
        "$$\n",
        "\\left\\{\\begin{array}{ll}\n",
        "\\mathcal{D}u(\\boldsymbol{\\boldsymbol{x}})=f(\\boldsymbol{\\boldsymbol{x}}), & \\text { in } \\Omega, \\\\\n",
        "\\mathcal{B}u(\\boldsymbol{\\boldsymbol{x}})=g(\\boldsymbol{\\boldsymbol{x}}), & \\text { on } \\partial \\Omega,\n",
        "\\end{array}\\right.\n",
        "$$\n",
        "\n",
        "where $\\partial \\Omega$ is the boundary of the domain, $\\mathcal{D}$ and $\\mathcal{B}$  are   differential operators in $\\Omega$ and on $\\partial \\Omega$, respectively. The goal is to train a neural network, denoted by $\\phi(\\boldsymbol{x}; \\boldsymbol{\\theta})$, where $\\boldsymbol{\\theta}$ is the set of network parameters, to approximate the ground truth solution  $u(\\boldsymbol{x})$ of the PDE.\n",
        "\n",
        "In the least squares method (LSM), the PDE is solved by finding the optimal set of parameters $\\boldsymbol{\\boldsymbol{\\theta}}^\\ast$ that minimizes the loss function; i.e.,\n",
        "\n",
        "$$\\begin{aligned}\n",
        "\\boldsymbol{\\theta}^\\ast = \\underset{\\boldsymbol{\\theta}}{\\arg \\min}\\text{ }\\mathcal{L}(\\boldsymbol{\\theta})\n",
        ":&=\\underset{\\boldsymbol{\\theta}}{\\arg \\min}\\text{ } \\|\\mathcal{D} \\phi(\\boldsymbol{x} ; \\boldsymbol{\\theta})-f(\\boldsymbol{x})\\|_{2}^{2}+\\lambda\\|\\mathcal{B} \\phi(\\boldsymbol{x} ; \\boldsymbol{\\theta})-g(\\boldsymbol{x})\\|_{2}^{2}\\\\\n",
        "&=\\underset{\\boldsymbol{\\theta}}{\\arg \\min}\\text{ } \\mathbb{E}_{\\boldsymbol{x} \\in \\Omega}\\left[|\\mathcal{D} \\phi(\\boldsymbol{x} ; \\boldsymbol{\\theta})-f(\\boldsymbol{x})|^{2}\\right]+\\lambda \\mathbb{E}_{\\boldsymbol{x} \\in \\partial \\Omega}\\left[|\\mathcal{B} \\phi(\\boldsymbol{x} ; \\boldsymbol{\\theta})-g(\\boldsymbol{x})|^{2}\\right]\\\\\n",
        "&\\approx \\underset{\\boldsymbol{\\theta}}{\\arg \\min}\\text{ } \\frac{1}{N_{1}} \\sum_{i=1}^{N_{1}}\\left|\\mathcal{D} \\phi\\left(\\boldsymbol{x}_{i} ; \\boldsymbol{\\theta}\\right)-f\\left(\\boldsymbol{x}_{i}\\right)\\right|^{2}+ \\frac{\\lambda}{N_{2}} \\sum_{j=1}^{N_{2}}\\left|\\mathcal{B} \\phi\\left(\\boldsymbol{x}_{j} ; \\boldsymbol{\\theta}\\right)-g\\left(\\boldsymbol{x}_{j}\\right)\\right|^{2},\n",
        "\\end{aligned}$$\n",
        "where $\\lambda$ is a positive hyper-parameter that weights the boundary loss. The last step is a Monte-Carlo approximation with $\\boldsymbol{x}_i \\in \\Omega$ and $\\boldsymbol{x}_j \\in \\partial \\Omega$ being $N_1$ and $N_2$ allocation points sampled from the respective probability densities that $\\boldsymbol{x}_i$ and $\\boldsymbol{x}_j$ follow. For a time-dependent PDE, the temporal coordinate can be regarded as another spatial coordinate to build a similar least squares model."
      ],
      "metadata": {
        "id": "5PhvJnUEIF7m"
      }
    }
  ]
}